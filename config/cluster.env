# ============================================================================
# K-Cloud MLPerf Benchmark Suite - Cluster Configuration
# ============================================================================
# Copy this file to config/cluster.env.local and edit with your values.
# The .local file is gitignored and will be used if present.
# ============================================================================

# Master Node Configuration
MASTER_IP=""
MASTER_USER=""
MASTER_HOSTNAME="master"

# Worker Node Configuration (GPU node)
# Format: user@ip or just ip (defaults to WORKER_USER)
WORKER_IP=""
WORKER_USER=""
WORKER_HOSTNAME="gpu-worker"
WORKER_SSH_PORT="22"

# Kubernetes Configuration
K8S_VERSION="1.28"
POD_NETWORK_CIDR="10.244.0.0/16"
SERVICE_CIDR="10.96.0.0/12"

# NVIDIA Configuration
NVIDIA_DRIVER_VERSION="550"  # Or "latest"
CUDA_VERSION="12.4"

# HuggingFace Configuration (required for Llama 3.1)
# Get your token from: https://huggingface.co/settings/tokens
HF_TOKEN=""

# Benchmark Configuration
BENCHMARK_NAMESPACE="mlperf"
DATA_DIR="/data"
HF_CACHE_DIR="/data/hf-cache"

# Container Images
VLLM_IMAGE="vllm/vllm-openai:v0.6.6"

# GPU Memory Settings
# Reduce max_model_len if GPU has less than 24GB VRAM
MAX_MODEL_LEN="4096"
GPU_MEMORY_UTILIZATION="0.9"
