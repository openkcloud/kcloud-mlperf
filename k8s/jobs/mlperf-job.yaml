# MLPerf Inference - CNN/DailyMail Summarization
# Official MLCommons-style evaluation with vLLM backend
# Python script: benchmarks/mlperf_summarization.py
apiVersion: batch/v1
kind: Job
metadata:
  name: mlperf-bench
  namespace: mlperf
spec:
  backoffLimit: 0
  template:
    spec:
      restartPolicy: Never
      runtimeClassName: nvidia
      nodeSelector:
        nvidia.com/gpu.present: "true"
      containers:
      - name: bench
        image: vllm/vllm-openai:v0.6.6
        command: ["bash", "-c"]
        args:
        - |
          set -e
          echo "════════════════════════════════════════════════════════════════════"
          echo " MLPerf Inference - Llama 3.1 8B"
          echo " (MLCommons-style evaluation with vLLM backend)"
          echo "════════════════════════════════════════════════════════════════════"
          pip install -q datasets evaluate rouge-score nltk
          python3 -c "import nltk; nltk.download('punkt', quiet=True); nltk.download('punkt_tab', quiet=True)"
          python3 -u /benchmarks/mlperf_summarization.py
        resources:
          limits:
            nvidia.com/gpu: "1"
            memory: "48Gi"
          requests:
            nvidia.com/gpu: "1"
            memory: "24Gi"
        env:
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token
              key: HF_TOKEN
        - name: HF_HOME
          value: /cache
        - name: TRANSFORMERS_CACHE
          value: /cache
        - name: SAMPLE_SPLIT
          value: "${SAMPLE_SPLIT}"
        - name: PYTHONUNBUFFERED
          value: "1"
        volumeMounts:
        - name: cache
          mountPath: /cache
        - name: benchmarks
          mountPath: /benchmarks
      volumes:
      - name: cache
        hostPath:
          path: /data/hf-cache
          type: DirectoryOrCreate
      - name: benchmarks
        configMap:
          name: benchmark-scripts
