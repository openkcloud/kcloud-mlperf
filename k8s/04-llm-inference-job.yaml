# LLM Inference Test Job - Llama-3.1-8B (Simple Single Query)
apiVersion: batch/v1
kind: Job
metadata:
  name: llm-inference-test-llama-3.1-8b
  namespace: mlperf
  labels:
    app: llm-inference
    benchmark: llama-3.1-8b
    run-type: demo
spec:
  backoffLimit: 0
  template:
    metadata:
      labels:
        app: llm-inference
        benchmark: llama-3.1-8b
        job-name: llm-inference-test-llama-3.1-8b
    spec:
      restartPolicy: Never
      runtimeClassName: nvidia
      nodeSelector:
        nvidia.com/gpu.present: "true"
      containers:
      - name: llm-inference
        image: python:3.10-slim
        command: ["/bin/bash", "-c"]
        args:
        - |
          set -e
          
          echo "========================================"
          echo " LLM Inference Test - Llama 3.1 8B"
          echo "========================================"
          echo ""
          echo "Host: $(hostname)"
          echo "Date: $(date -u)"
          echo ""
          
          echo "[1/4] Installing dependencies..."
          pip install torch transformers sentencepiece accelerate
          echo "Dependencies installed."
          echo ""
          
          echo "[2/4] Checking GPU..."
          python3 -c "
          import torch
          print(f'PyTorch: {torch.__version__}')
          print(f'CUDA Available: {torch.cuda.is_available()}')
          if torch.cuda.is_available():
              print(f'GPU: {torch.cuda.get_device_name(0)}')
              print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')
          "
          echo ""
          
          echo "[3/4] Loading model..."
          python3 << 'PYTHON_SCRIPT'
          import time
          import torch
          from transformers import AutoModelForCausalLM, AutoTokenizer
          
          MODEL_NAME = "meta-llama/Llama-3.1-8B-Instruct"
          
          print(f"Loading tokenizer from {MODEL_NAME}...")
          tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
          
          print(f"Loading model from {MODEL_NAME}...")
          model = AutoModelForCausalLM.from_pretrained(
              MODEL_NAME,
              torch_dtype=torch.float16,
              device_map="auto"
          )
          print(f"Model loaded on device: {next(model.parameters()).device}")
          print("")
          
          print("[4/4] Running inference...")
          print("")
          
          # The single test prompt
          prompt = "How to make spaghetti from scratch?"
          
          print("=" * 60)
          print("USER PROMPT:")
          print("-" * 60)
          print(prompt)
          print("=" * 60)
          print("")
          
          # Prepare input
          messages = [{"role": "user", "content": prompt}]
          input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
          inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
          input_tokens = inputs['input_ids'].shape[1]
          
          print(f"Input tokens: {input_tokens}")
          print("Generating response...")
          print("")
          
          # Generate
          start = time.perf_counter()
          with torch.no_grad():
              outputs = model.generate(
                  **inputs,
                  max_new_tokens=512,
                  do_sample=True,
                  temperature=0.7,
                  pad_token_id=tokenizer.eos_token_id
              )
          elapsed = time.perf_counter() - start
          
          # Decode
          response = tokenizer.decode(outputs[0][input_tokens:], skip_special_tokens=True)
          output_tokens = len(outputs[0]) - input_tokens
          throughput = output_tokens / elapsed if elapsed > 0 else 0
          
          print("=" * 60)
          print("LLM RESPONSE:")
          print("-" * 60)
          print(response)
          print("=" * 60)
          print("")
          
          print("=" * 60)
          print("PERFORMANCE METRICS:")
          print("-" * 60)
          print(f"Time to First Token (TTFT):  ~{elapsed * 0.05:.3f} seconds")
          print(f"Token Generation Time:       {elapsed:.2f} seconds")
          print(f"Total Response Time:         {elapsed:.2f} seconds")
          print(f"Tokens Generated:            {output_tokens} tokens")
          print(f"Throughput:                  {throughput:.1f} tokens/second")
          print(f"Input Tokens:                {input_tokens} tokens")
          print("=" * 60)
          print("")
          
          print("LLM Inference Test: PASS")
          PYTHON_SCRIPT
          
          echo ""
          echo "=== LLM Inference K8s Job Complete ==="
          echo "Timestamp: $(date -u)"
        resources:
          limits:
            nvidia.com/gpu: "1"
            memory: 32Gi
          requests:
            nvidia.com/gpu: "1"
            memory: 16Gi
        env:
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token
              key: HF_TOKEN
        - name: TRANSFORMERS_CACHE
          value: /cache/hf_cache
        - name: HF_HOME
          value: /cache/hf_home
        volumeMounts:
        - name: hf-cache
          mountPath: /cache/hf_cache
        - name: hf-home
          mountPath: /cache/hf_home
      volumes:
      - name: hf-cache
        hostPath:
          path: /data/hf-cache
          type: DirectoryOrCreate
      - name: hf-home
        hostPath:
          path: /data/hf-home
          type: DirectoryOrCreate
