# MMLU-Pro Llama-3.1-8B Benchmark Job (FULL DATASET)
apiVersion: batch/v1
kind: Job
metadata:
  name: mmlu-pro-llama-3.1-8b
  namespace: mlperf
  labels:
    app: mmlu-pro
    benchmark: llama-3.1-8b
    run-type: full
spec:
  backoffLimit: 0
  template:
    metadata:
      labels:
        app: mmlu-pro
        benchmark: llama-3.1-8b
        job-name: mmlu-pro-llama-3.1-8b
    spec:
      restartPolicy: Never
      runtimeClassName: nvidia
      nodeSelector:
        nvidia.com/gpu.present: "true"
      containers:
      - name: mmlu-pro-runner
        image: python:3.10-slim
        command: ["/bin/bash", "-c"]
        args:
        - |
          set -e
          
          echo "========================================"
          echo " MMLU-Pro - Llama 3.1 8B"
          echo " FULL DATASET (12,032 questions)"
          echo "========================================"
          echo ""
          echo "Host: $(hostname)"
          echo "Date: $(date -u)"
          echo ""
          
          echo "[1/5] Installing dependencies..."
          pip install torch transformers datasets tqdm sentencepiece accelerate
          echo "Dependencies installed."
          echo ""
          
          echo "[2/5] Checking GPU..."
          python3 -c "
          import torch
          print(f'PyTorch: {torch.__version__}')
          print(f'CUDA Available: {torch.cuda.is_available()}')
          if torch.cuda.is_available():
              print(f'GPU: {torch.cuda.get_device_name(0)}')
              print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')
          "
          echo ""
          
          echo "[3/5] Loading model and dataset..."
          python3 << 'PYTHON_SCRIPT'
          import os
          import time
          import re
          import sys
          from collections import defaultdict
          
          import torch
          from datasets import load_dataset
          from transformers import AutoModelForCausalLM, AutoTokenizer
          
          print("Loading MMLU-Pro FULL dataset...")
          dataset = load_dataset("TIGER-Lab/MMLU-Pro", split="test")
          total_samples = len(dataset)
          print(f"Loaded {total_samples} samples")
          
          print("Loading Llama 3.1 8B model...")
          MODEL_NAME = "meta-llama/Llama-3.1-8B-Instruct"
          tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
          model = AutoModelForCausalLM.from_pretrained(
              MODEL_NAME,
              torch_dtype=torch.float16,
              device_map="auto"
          )
          print(f"Model loaded on: {next(model.parameters()).device}")
          print("")
          
          print(f"[4/5] Running MMLU evaluation on {total_samples} questions...")
          print("This will take several hours. Progress updates every 500 questions.")
          print("")
          
          correct = 0
          total_time = 0
          failed = 0
          category_stats = defaultdict(lambda: {'correct': 0, 'total': 0})
          
          start_total = time.perf_counter()
          
          for i, sample in enumerate(dataset):
              try:
                  question = sample["question"]
                  options = sample["options"]
                  answer = sample["answer"]
                  category = sample.get("category", "unknown")
                  
                  # Format options
                  option_str = "\n".join([f"{chr(65+j)}. {opt}" for j, opt in enumerate(options)])
                  
                  prompt = f"""Answer the following multiple choice question. Reply with ONLY the letter of the correct answer (A, B, C, etc).

          Question: {question}

          {option_str}

          Answer:"""
                  
                  messages = [{"role": "user", "content": prompt}]
                  input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                  inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=2048).to(model.device)
                  
                  start = time.perf_counter()
                  with torch.no_grad():
                      outputs = model.generate(**inputs, max_new_tokens=10, do_sample=False, pad_token_id=tokenizer.eos_token_id)
                  elapsed = time.perf_counter() - start
                  total_time += elapsed
                  
                  response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip().upper()
                  
                  # Extract letter from response
                  match = re.search(r'[A-J]', response)
                  pred = match.group(0) if match else ""
                  
                  is_correct = (pred == answer)
                  if is_correct:
                      correct += 1
                      category_stats[category]['correct'] += 1
                  category_stats[category]['total'] += 1
                  
                  # Progress update every 500 questions
                  if (i + 1) % 500 == 0:
                      elapsed_total = time.perf_counter() - start_total
                      current_acc = correct / (i + 1)
                      rate = (i + 1) / elapsed_total
                      eta = (total_samples - i - 1) / rate if rate > 0 else 0
                      print(f"  [{i+1}/{total_samples}] Acc: {current_acc:.2%}, {elapsed_total/60:.1f}m elapsed, {rate:.1f} q/s, ETA: {eta/60:.0f}m")
                      sys.stdout.flush()
                      
              except Exception as e:
                  print(f"  [ERROR] Q{i}: {str(e)[:50]}")
                  failed += 1
                  category_stats[sample.get("category", "unknown")]['total'] += 1
          
          total_elapsed = time.perf_counter() - start_total
          completed = total_samples - failed
          accuracy = correct / completed if completed > 0 else 0
          
          print("")
          print(f"Evaluation complete: {completed}/{total_samples} questions in {total_elapsed/60:.1f} minutes")
          print("")
          
          print("[5/5] Generating results summary...")
          print("")
          
          # Category breakdown
          print("=== Category Breakdown ===")
          sorted_cats = sorted(category_stats.items(), key=lambda x: x[1]['total'], reverse=True)
          for cat, stats in sorted_cats[:10]:  # Top 10 categories
              cat_acc = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
              print(f"  {cat}: {cat_acc:.2%} ({stats['correct']}/{stats['total']})")
          if len(sorted_cats) > 10:
              print(f"  ... and {len(sorted_cats) - 10} more categories")
          print("")
          
          print("=== Results Summary (MMLU) ===")
          print(f"Total questions processed: {total_samples}")
          print(f"Questions completed: {completed}")
          print(f"Questions failed: {failed}")
          print(f"Completion rate: {completed/total_samples*100:.2f}%")
          print(f"Overall accuracy: {accuracy:.4f}")
          print(f"Correct answers: {correct}/{completed}")
          print(f"Average response time: {total_time/max(completed,1):.2f}s")
          hours = int(total_elapsed // 3600)
          mins = int((total_elapsed % 3600) // 60)
          secs = int(total_elapsed % 60)
          print(f"Total processing time: {hours}h{mins}m{secs}s")
          print(f"Model: meta-llama/Llama-3.1-8B-Instruct")
          print(f"Backend: hf-transformers")
          print(f"Dataset: MMLU-Pro (full test set)")
          print("")
          
          print("=== Acceptance Criteria (MMLU) ===")
          print(f"Required overall accuracy: >= 0.3500")
          print(f"Required completion rate:  >= 99.00%")
          print("")
          print(f"Observed overall accuracy: {accuracy:.4f}")
          print(f"Observed completion rate:  {completed/total_samples*100:.2f}%")
          print("")
          
          if accuracy >= 0.35 and (completed/total_samples) >= 0.99:
              print("MMLU Benchmark Status: PASS")
          else:
              print("MMLU Benchmark Status: FAIL")
              if accuracy < 0.40:
                  print(f"  - Accuracy below threshold")
              if (completed/total_samples) < 0.99:
                  print(f"  - Completion rate below threshold")
              exit(1)
          PYTHON_SCRIPT
          
          echo ""
          echo "=== MMLU-Pro K8s Job Complete ==="
          echo "Timestamp: $(date -u)"
        resources:
          limits:
            nvidia.com/gpu: "1"
            memory: 48Gi
          requests:
            nvidia.com/gpu: "1"
            memory: 24Gi
        env:
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token
              key: HF_TOKEN
        - name: TRANSFORMERS_CACHE
          value: /cache/hf_cache
        - name: HF_HOME
          value: /cache/hf_home
        volumeMounts:
        - name: hf-cache
          mountPath: /cache/hf_cache
        - name: hf-home
          mountPath: /cache/hf_home
      volumes:
      - name: hf-cache
        hostPath:
          path: /data/hf-cache
          type: DirectoryOrCreate
      - name: hf-home
        hostPath:
          path: /data/hf-home
          type: DirectoryOrCreate
