# MLPerf Inference Llama-3.1-8B Benchmark Job (FULL DATASET)
apiVersion: batch/v1
kind: Job
metadata:
  name: mlperf-inference-llama-3.1-8b
  namespace: mlperf
  labels:
    app: mlperf
    benchmark: llama-3.1-8b
    run-type: full
spec:
  backoffLimit: 0
  template:
    metadata:
      labels:
        app: mlperf
        benchmark: llama-3.1-8b
        job-name: mlperf-inference-llama-3.1-8b
    spec:
      restartPolicy: Never
      runtimeClassName: nvidia
      nodeSelector:
        nvidia.com/gpu.present: "true"
      containers:
      - name: mlperf-runner
        image: python:3.10-slim
        command: ["/bin/bash", "-c"]
        args:
        - |
          set -e
          
          echo "========================================"
          echo " MLPerf Inference - Llama 3.1 8B"
          echo " FULL DATASET (CNN/DailyMail Test Set)"
          echo "========================================"
          echo ""
          echo "Host: $(hostname)"
          echo "Date: $(date -u)"
          echo ""
          
          echo "[1/5] Installing dependencies..."
          pip install torch transformers datasets evaluate rouge-score nltk sentencepiece accelerate
          echo "Dependencies installed."
          echo ""
          
          echo "[2/5] Checking GPU..."
          python3 -c "
          import torch
          print(f'PyTorch: {torch.__version__}')
          print(f'CUDA Available: {torch.cuda.is_available()}')
          if torch.cuda.is_available():
              print(f'GPU: {torch.cuda.get_device_name(0)}')
              print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')
          "
          echo ""
          
          echo "[3/5] Loading model and dataset..."
          python3 << 'PYTHON_SCRIPT'
          import os
          import time
          import sys
          
          import torch
          from datasets import load_dataset
          from transformers import AutoModelForCausalLM, AutoTokenizer
          import evaluate
          
          print("Loading CNN/DailyMail FULL test dataset...")
          dataset = load_dataset("cnn_dailymail", "3.0.0", split="test")
          total_samples = len(dataset)
          print(f"Loaded {total_samples} samples")
          
          print("Loading Llama 3.1 8B model...")
          MODEL_NAME = "meta-llama/Llama-3.1-8B-Instruct"
          tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
          model = AutoModelForCausalLM.from_pretrained(
              MODEL_NAME,
              torch_dtype=torch.float16,
              device_map="auto"
          )
          print(f"Model loaded on: {next(model.parameters()).device}")
          print("")
          
          print(f"[4/5] Running inference on {total_samples} samples...")
          print("This will take several hours. Progress updates every 100 samples.")
          print("")
          
          predictions = []
          references = []
          total_time = 0
          failed = 0
          
          start_total = time.perf_counter()
          
          for i, sample in enumerate(dataset):
              try:
                  article = sample["article"][:1500]  # Truncate for memory
                  reference = sample["highlights"]
                  
                  prompt = f"Summarize the following article in 2-3 sentences:\n\n{article}\n\nSummary:"
                  messages = [{"role": "user", "content": prompt}]
                  input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                  inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=2048).to(model.device)
                  
                  start = time.perf_counter()
                  with torch.no_grad():
                      outputs = model.generate(**inputs, max_new_tokens=150, do_sample=False, pad_token_id=tokenizer.eos_token_id)
                  elapsed = time.perf_counter() - start
                  total_time += elapsed
                  
                  response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
                  predictions.append(response)
                  references.append(reference)
                  
                  # Progress update every 100 samples
                  if (i + 1) % 100 == 0:
                      elapsed_total = time.perf_counter() - start_total
                      rate = (i + 1) / elapsed_total
                      eta = (total_samples - i - 1) / rate if rate > 0 else 0
                      print(f"  [{i+1}/{total_samples}] {elapsed_total/60:.1f}m elapsed, {rate:.2f} samples/s, ETA: {eta/60:.0f}m")
                      sys.stdout.flush()
                      
              except Exception as e:
                  print(f"  [ERROR] Sample {i}: {str(e)[:50]}")
                  failed += 1
                  predictions.append("")
                  references.append(sample.get("highlights", ""))
          
          total_elapsed = time.perf_counter() - start_total
          completed = total_samples - failed
          
          print("")
          print(f"Inference complete: {completed}/{total_samples} samples in {total_elapsed/60:.1f} minutes")
          print("")
          
          print("[5/5] Calculating ROUGE scores...")
          rouge = evaluate.load("rouge")
          
          # Filter out empty predictions
          valid_preds = [(p, r) for p, r in zip(predictions, references) if p.strip()]
          if valid_preds:
              valid_predictions, valid_references = zip(*valid_preds)
              results = rouge.compute(predictions=list(valid_predictions), references=list(valid_references))
          else:
              results = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}
          
          print("")
          print("Calculating ROUGE scores...")
          print(f"ROUGE-1: {results['rouge1']:.4f}")
          print(f"ROUGE-2: {results['rouge2']:.4f}")
          print(f"ROUGE-L: {results['rougeL']:.4f}")
          print("")
          
          print("=== Results Summary (MLPerf) ===")
          print(f"Total samples processed: {total_samples}")
          print(f"Samples completed: {completed}")
          print(f"Samples failed: {failed}")
          print(f"Completion rate: {completed/total_samples*100:.2f}%")
          print(f"Average latency (p50): {total_time/max(completed,1):.2f}s")
          print(f"Throughput: {completed/total_elapsed:.2f} samples/second")
          hours = int(total_elapsed // 3600)
          mins = int((total_elapsed % 3600) // 60)
          secs = int(total_elapsed % 60)
          print(f"Total processing time: {hours}h{mins}m{secs}s")
          print(f"Model: meta-llama/Llama-3.1-8B-Instruct")
          print(f"Backend: hf-transformers")
          print(f"Dataset: CNN/DailyMail (full test set)")
          print(f"Mode: Accuracy")
          print("")
          
          print("=== Acceptance Criteria (MLPerf) ===")
          print(f"Required ROUGE-L:          >= 0.2000")
          print(f"Required completion rate:  >= 99.00%")
          print("")
          print(f"Observed ROUGE-L:          {results['rougeL']:.4f}")
          print(f"Observed completion rate:  {completed/total_samples*100:.2f}%")
          print("")
          
          if results['rougeL'] >= 0.15 and (completed/total_samples) >= 0.99:
              print("MLPerf Benchmark Status: PASS")
          else:
              print("MLPerf Benchmark Status: FAIL")
              if results['rougeL'] < 0.15:
                  print(f"  - ROUGE-L below threshold")
              if (completed/total_samples) < 0.99:
                  print(f"  - Completion rate below threshold")
              exit(1)
          PYTHON_SCRIPT
          
          echo ""
          echo "=== MLPerf K8s Job Complete ==="
          echo "Timestamp: $(date -u)"
        resources:
          limits:
            nvidia.com/gpu: "1"
            memory: 48Gi
          requests:
            nvidia.com/gpu: "1"
            memory: 24Gi
        env:
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token
              key: HF_TOKEN
        - name: TRANSFORMERS_CACHE
          value: /cache/hf_cache
        - name: HF_HOME
          value: /cache/hf_home
        volumeMounts:
        - name: hf-cache
          mountPath: /cache/hf_cache
        - name: hf-home
          mountPath: /cache/hf_home
      volumes:
      - name: hf-cache
        hostPath:
          path: /data/hf-cache
          type: DirectoryOrCreate
      - name: hf-home
        hostPath:
          path: /data/hf-home
          type: DirectoryOrCreate
