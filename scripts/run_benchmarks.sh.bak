#!/bin/bash
# ============================================================================
# run_benchmarks.sh - K-Cloud LLM Benchmark Suite
# ============================================================================
set -eo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"
K8S_DIR="$PROJECT_DIR/k8s"
NAMESPACE="mlperf"

# Defaults
SMOKE_TEST=false
RUN_MLPERF=true
RUN_MMLU=true
RUN_INFERENCE=true
HF_TOKEN="${HF_TOKEN:-}"
RUN_ID="$(date +%Y%m%d-%H%M%S)"
RESULTS_DIR="$PROJECT_DIR/results/$RUN_ID"
SUMMARY_FILE="$RESULTS_DIR/summary.txt"

for arg in "$@"; do
    case $arg in
        --smoke) SMOKE_TEST=true ;;
        --mlperf) RUN_MMLU=false; RUN_INFERENCE=false ;;
        --mmlu) RUN_MLPERF=false; RUN_INFERENCE=false ;;
        --inference) RUN_MLPERF=false; RUN_MMLU=false ;;
        --help|-h)
            echo "Usage: $0 [--smoke] [--mlperf|--mmlu|--inference]"
            echo "  --smoke      Quick test with 10 samples (~15 min)"
            echo "  --mlperf     Run only MLPerf"
            echo "  --mmlu       Run only MMLU-Pro"
            echo "  --inference  Run only LLM Inference"
            exit 0 ;;
    esac
done

echo ""
echo "╔══════════════════════════════════════════════════════════════════╗"
echo "║   K-Cloud LLM Benchmark Suite - Llama 3.1 8B                     ║"
echo "╚══════════════════════════════════════════════════════════════════╝"
echo ""
if [ "$SMOKE_TEST" = true ]; then
    echo "Mode: SMOKE TEST (10 samples each, ~15 min)"
    SAMPLE_SPLIT="[:10]"
else
    echo "Mode: FULL DATASET (8-10 hours)"
    echo "  - MLPerf: ~11k samples (CNN/DailyMail test set)"
    echo "  - MMLU-Pro: ~12k questions"
    echo "  - Progress updates: every 1% or 60 seconds"
    echo "  - Heartbeat status: every 2 minutes during job execution"
    SAMPLE_SPLIT=""
fi
echo "Date: $(date)"
echo ""
mkdir -p "$RESULTS_DIR"
{
    echo "K-Cloud LLM Benchmark Suite"
    echo "Run ID: $RUN_ID"
    echo "Date: $(date)"
    echo "Mode: $([ "$SMOKE_TEST" = true ] && echo 'Smoke Test' || echo 'Full Dataset')"
    echo "Components: MLPerf=$RUN_MLPERF MMLU=$RUN_MMLU Inference=$RUN_INFERENCE"
    echo ""
} > "$SUMMARY_FILE"

status() {
    # Timestamped status line for better live visibility
    echo "[$(date '+%H:%M:%S')] $*"
}

check_runtime_and_gpu() {
    status "[1a] Validating GPU runtime..."
    if ! kubectl get runtimeclass nvidia >/dev/null 2>&1; then
        echo "ERROR: RuntimeClass 'nvidia' not found. Install NVIDIA runtime or remove runtimeClassName from the job spec."
        exit 1
    fi

    local gpu_nodes
    gpu_nodes=$(kubectl get nodes -l nvidia.com/gpu.present=true --no-headers 2>/dev/null | wc -l | xargs)
    if [ "${gpu_nodes:-0}" -eq 0 ]; then
        echo "ERROR: No GPU nodes labeled with nvidia.com/gpu.present=true"
        exit 1
    fi
    echo "✓ RuntimeClass 'nvidia' present; GPU nodes available: $gpu_nodes"
    echo ""
}

ensure_hf_secret() {
    if kubectl get secret hf-token -n $NAMESPACE >/dev/null 2>&1; then
        local placeholder current
        placeholder=$(echo -n "YOUR_HUGGINGFACE_TOKEN" | base64)
        current=$(kubectl get secret hf-token -n $NAMESPACE -o jsonpath='{.data.HF_TOKEN}' 2>/dev/null || echo "")

        if [ "$current" = "$placeholder" ]; then
            if [ -z "$HF_TOKEN" ]; then
                echo "ERROR: hf-token secret contains placeholder. Export HF_TOKEN to recreate it."
                exit 1
            fi
            echo "Updating hf-token secret with provided HF_TOKEN..."
            kubectl delete secret hf-token -n $NAMESPACE >/dev/null 2>&1 || true
            kubectl create secret generic hf-token --from-literal=HF_TOKEN="$HF_TOKEN" -n $NAMESPACE
        fi
    else
        if [ -z "$HF_TOKEN" ]; then
            echo "ERROR: Set HF_TOKEN env var"
            exit 1
        fi
        kubectl create secret generic hf-token --from-literal=HF_TOKEN="$HF_TOKEN" -n $NAMESPACE
    fi
}

# Check cluster
status "[1/4] Checking Cluster..."
kubectl cluster-info > /dev/null || { echo "ERROR: Cannot connect to cluster"; exit 1; }
echo "✓ Cluster OK"
kubectl get nodes -o wide
echo ""
check_runtime_and_gpu

# Setup
status "[2/4] Setting up namespace..."
kubectl apply -f "$K8S_DIR/00-namespace.yaml" 2>/dev/null || true
ensure_hf_secret
echo "✓ Namespace and hf-token secret ready"
echo ""

# Function to run job and wait for completion
run_job() {
    local job_name=$1
    local description=$2
    local yaml_content=$3
    local log_file="$RESULTS_DIR/${job_name}.log"
    local diag_file="$RESULTS_DIR/${job_name}-diagnostics.log"
    local manifest_file="$RESULTS_DIR/${job_name}-manifest.yaml"
    
    echo "════════════════════════════════════════════════════════════════════"
    echo "  $description"
    echo "════════════════════════════════════════════════════════════════════"
    echo "$yaml_content" > "$manifest_file"
    status "Saved manifest -> $manifest_file"
    
    # Delete existing job
    status "Deleting any existing job $job_name..."
    kubectl delete job $job_name -n $NAMESPACE --ignore-not-found=true 2>/dev/null
    sleep 3
    
    # Apply job
    status "Creating job $job_name..."
    echo "$yaml_content" | kubectl apply -f -
    
    # Wait for pod to start
    echo "Waiting for pod to start..."
    local pod=""
    local status=""
    for i in $(seq 1 120); do
        pod=$(kubectl get pods -n $NAMESPACE -l job-name=$job_name -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
        if [ -n "$pod" ]; then
            status=$(kubectl get pod $pod -n $NAMESPACE -o jsonpath='{.status.phase}' 2>/dev/null)
            if [ "$status" = "Running" ]; then
                echo "Pod $pod is Running on $(kubectl get pod $pod -n $NAMESPACE -o jsonpath='{.spec.nodeName}')"
                break
            elif [ "$status" = "Succeeded" ] || [ "$status" = "Failed" ]; then
                echo "Pod $pod finished with status: $status"
                break
            else
                status "Pod $pod state: $status (waiting...)"

                # Fail fast if scheduler already marked it unschedulable
                pod_scheduled_status=$(kubectl get pod $pod -n $NAMESPACE -o jsonpath='{.status.conditions[?(@.type=="PodScheduled")].status}' 2>/dev/null || true)
                pod_scheduled_reason=$(kubectl get pod $pod -n $NAMESPACE -o jsonpath='{.status.conditions[?(@.type=="PodScheduled")].reason}' 2>/dev/null || true)
                if [ "$pod_scheduled_status" = "False" ] && [ "$pod_scheduled_reason" = "Unschedulable" ]; then
                    echo "ERROR: Pod $pod is Unschedulable"
                    kubectl describe pod $pod -n $NAMESPACE || true
                    kubectl get events -n $NAMESPACE --sort-by=.metadata.creationTimestamp | tail -n 20 || true
                    {
                        echo "[$(date '+%F %T')] $description unschedulable"
                        kubectl describe pod $pod -n $NAMESPACE
                        kubectl get events -n $NAMESPACE --sort-by=.metadata.creationTimestamp | tail -n 20
                    } > "$diag_file" 2>&1 || true
                    echo "FAIL $description" >> "$SUMMARY_FILE"
                    return 1
                fi
            fi
        fi
        status "Waiting for pod... attempt $i/120"
        sleep 5
    done
    
    if [ -z "$pod" ]; then
        echo "ERROR: Pod failed to start"
        kubectl describe job $job_name -n $NAMESPACE
        kubectl get events -n $NAMESPACE --sort-by=.metadata.creationTimestamp | tail -n 20
        {
            echo "[$(date '+%F %T')] $description pod failed to start"
            kubectl describe job $job_name -n $NAMESPACE
            kubectl get events -n $NAMESPACE --sort-by=.metadata.creationTimestamp | tail -n 20
        } > "$diag_file" 2>&1
        echo "FAIL $description" >> "$SUMMARY_FILE"
        return 1
    fi

    # If we exited the loop with a pod but it's still Pending, treat as failure
    if [ "$status" = "Pending" ]; then
        echo "ERROR: Pod $pod is still Pending after waiting"
        kubectl describe pod $pod -n $NAMESPACE || true
        kubectl get events -n $NAMESPACE --sort-by=.metadata.creationTimestamp | tail -n 20 || true
        {
            echo "[$(date '+%F %T')] $description still pending after timeout"
            kubectl describe pod $pod -n $NAMESPACE
            kubectl get events -n $NAMESPACE --sort-by=.metadata.creationTimestamp | tail -n 20
        } > "$diag_file" 2>&1 || true
        echo "FAIL $description" >> "$SUMMARY_FILE"
        return 1
    fi
    
    # Stream logs until pod completes
    echo ""
    echo "--- Streaming logs (real-time) ---"
    status "Log file: $log_file"
    
    # Use stdbuf for line-buffered output, fall back to unbuffered if stdbuf unavailable
    if command -v stdbuf >/dev/null 2>&1; then
        stdbuf -oL kubectl logs -f $pod -n $NAMESPACE 2>&1 | stdbuf -oL tee -a "$log_file" &
    else
        kubectl logs -f $pod -n $NAMESPACE 2>&1 | tee -a "$log_file" &
    fi
    local logs_pid=$!
    
    # Wait for job to complete (poll every 10 seconds, with active log refresh)
    echo ""
    local poll_count=0
    local start_time=$(date +%s)
    local last_log_lines=0
    while true; do
        sleep 10
        poll_count=$((poll_count + 1))
        
        # Every 30 seconds, show current progress by tailing recent logs
        if [ $((poll_count % 3)) -eq 0 ]; then
            local now=$(date +%s)
            local elapsed_min=$(( (now - start_time) / 60 ))
            local pod_status_now=$(kubectl get pod $pod -n $NAMESPACE -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
            
            # Fetch and display recent log lines (last 5 lines with progress info)
            local recent_progress=$(kubectl logs $pod -n $NAMESPACE --tail=10 2>/dev/null | grep -E '^\s*\[|samples/s|q/s|ETA:|%' | tail -3)
            if [ -n "$recent_progress" ]; then
                echo ""
                status "[Progress Update] Elapsed: ${elapsed_min}m | Pod: $pod_status_now"
                echo "$recent_progress"
            else
                status "[Heartbeat] Job: $job_name | Pod: $pod_status_now | Elapsed: ${elapsed_min}m"
            fi
        fi
        
        # Check job status
        succeeded=$(kubectl get job $job_name -n $NAMESPACE -o jsonpath='{.status.succeeded}' 2>/dev/null)
        failed=$(kubectl get job $job_name -n $NAMESPACE -o jsonpath='{.status.failed}' 2>/dev/null)
        
        if [ "$succeeded" = "1" ]; then
            kill $logs_pid 2>/dev/null || true
            wait $logs_pid 2>/dev/null || true
            echo ""
            echo "✓ $description: PASS"
            echo "PASS $description" >> "$SUMMARY_FILE"
            return 0
        elif [ "$failed" = "1" ]; then
            kill $logs_pid 2>/dev/null || true
            wait $logs_pid 2>/dev/null || true
            echo ""
            echo "✗ $description: FAIL"
            kubectl describe pod $pod -n $NAMESPACE || true
            kubectl logs $pod -n $NAMESPACE --previous || true
            kubectl get events -n $NAMESPACE --sort-by=.metadata.creationTimestamp | tail -n 20 || true
            {
                echo "[$(date '+%F %T')] $description failed"
                kubectl describe pod $pod -n $NAMESPACE
                kubectl logs $pod -n $NAMESPACE --previous
                kubectl get events -n $NAMESPACE --sort-by=.metadata.creationTimestamp | tail -n 20
            } > "$diag_file" 2>&1 || true
            echo "FAIL $description" >> "$SUMMARY_FILE"
            return 1
        fi
        
        # Check if pod still exists and is running
        pod_status=$(kubectl get pod $pod -n $NAMESPACE -o jsonpath='{.status.phase}' 2>/dev/null)
        if [ "$pod_status" = "Succeeded" ]; then
            kill $logs_pid 2>/dev/null || true
            wait $logs_pid 2>/dev/null || true
            echo ""
            echo "✓ $description: PASS"
            return 0
        elif [ "$pod_status" = "Failed" ]; then
            kill $logs_pid 2>/dev/null || true
            wait $logs_pid 2>/dev/null || true
            echo ""
            echo "✗ $description: FAIL"
            kubectl describe pod $pod -n $NAMESPACE || true
            kubectl logs $pod -n $NAMESPACE --previous || true
            kubectl get events -n $NAMESPACE --sort-by=.metadata.creationTimestamp | tail -n 20 || true
            {
                echo "[$(date '+%F %T')] $description failed"
                kubectl describe pod $pod -n $NAMESPACE
                kubectl logs $pod -n $NAMESPACE --previous
                kubectl get events -n $NAMESPACE --sort-by=.metadata.creationTimestamp | tail -n 20
            } > "$diag_file" 2>&1 || true
            echo "FAIL $description" >> "$SUMMARY_FILE"
            return 1
        fi
    done
}

# Results array
declare -a RESULTS

echo "[3/4] Running Benchmarks..."
echo ""

# MLPerf - Official MLCommons implementation with vLLM
# Based on: mlcommons_inference/language/llama3.1-8b/
if [ "$RUN_MLPERF" = true ]; then
    MLPERF_YAML=$(cat <<'EOFYAML'
apiVersion: batch/v1
kind: Job
metadata:
  name: mlperf-bench
  namespace: mlperf
spec:
  backoffLimit: 0
  template:
    spec:
      restartPolicy: Never
      runtimeClassName: nvidia
      nodeSelector:
        nvidia.com/gpu.present: "true"
      containers:
      - name: bench
        image: vllm/vllm-openai:v0.6.6
        command: ["bash", "-c"]
        args:
        - |
          set -e
          echo "========================================"
          echo " MLPerf Inference - Llama 3.1 8B"
          echo " (Official MLCommons evaluation with vLLM)"
          echo "========================================"
          pip install -q datasets evaluate rouge-score nltk
          python3 -c "import nltk; nltk.download('punkt', quiet=True); nltk.download('punkt_tab', quiet=True)"
          cat > /tmp/bench.py << 'PYEOF'
          import time, sys, torch, nltk, os
          from datasets import load_dataset
          from vllm import LLM, SamplingParams
          import evaluate
          SPLIT = os.environ.get('SAMPLE_SPLIT', '')
          print('=' * 60, flush=True)
          print('MLPerf Inference Benchmark - Official Evaluation', flush=True)
          print('Model: meta-llama/Llama-3.1-8B-Instruct', flush=True)
          print('Backend: vLLM (batched inference)', flush=True)
          print('=' * 60, flush=True)
          print('[1/4] Loading dataset (CNN/DailyMail)...', flush=True)
          load_start = time.time()
          dataset = load_dataset('cnn_dailymail', '3.0.0', split=f'test{SPLIT}')
          total = len(dataset)
          print(f'  Dataset loaded: {total} samples in {time.time()-load_start:.1f}s', flush=True)
          print('[2/4] Loading model with vLLM...', flush=True)
          model_start = time.time()
          llm = LLM(model='meta-llama/Llama-3.1-8B-Instruct', dtype='float16', gpu_memory_utilization=0.9, max_model_len=4096, trust_remote_code=True)
          print(f'  Model loaded in {time.time()-model_start:.1f}s', flush=True)
          print(f'  GPU: {torch.cuda.get_device_name(0)}', flush=True)
          print('[3/4] Running inference with vLLM batching...', flush=True)
          prompts, refs = [], []
          for s in dataset:
              article = s['article'][:2000]
              prompts.append(f'Summarize the following article in a few sentences.\n\nArticle: {article}\n\nSummary:')
              refs.append(s['highlights'])
          sampling_params = SamplingParams(temperature=0, max_tokens=150, stop=['\n\n'])
          t0 = time.time()
          batch_size = 32
          preds = []
          for i in range(0, len(prompts), batch_size):
              batch = prompts[i:i+batch_size]
              outputs = llm.generate(batch, sampling_params)
              for output in outputs:
                  preds.append(output.outputs[0].text.strip())
              done = min(i + batch_size, len(prompts))
              elapsed = time.time() - t0
              rate = done / elapsed if elapsed > 0 else 0
              eta = (total - done) / rate / 60 if rate > 0 else 0
              print(f'  [{done}/{total}] {done*100/total:.1f}% | {rate:.1f} samples/s | ETA: {eta:.0f}m', flush=True)
          elapsed = time.time() - t0
          print(f'  Completed {total} samples in {elapsed/60:.1f} minutes', flush=True)
          print('[4/4] Computing ROUGE scores...', flush=True)
          def postprocess(preds, targets):
              preds = ['\n'.join(nltk.sent_tokenize(p.strip())) for p in preds]
              targets = ['\n'.join(nltk.sent_tokenize(t.strip())) for t in targets]
              return preds, targets
          preds_p, refs_p = postprocess(preds, refs)
          rouge = evaluate.load('rouge')
          result = rouge.compute(predictions=preds_p, references=refs_p, use_stemmer=True)
          print('=' * 60, flush=True)
          print('RESULTS (Official MLCommons Metrics)', flush=True)
          print(f"ROUGE-1: {result['rouge1']:.4f}", flush=True)
          print(f"ROUGE-2: {result['rouge2']:.4f}", flush=True)
          print(f"ROUGE-L: {result['rougeL']:.4f}", flush=True)
          print(f'Samples: {total} | Time: {elapsed/60:.1f}m | Throughput: {total/elapsed:.2f} samples/s', flush=True)
          status = 'PASS' if result['rougeL'] >= 0.15 else 'FAIL'
          print(f'Status: {status} (ROUGE-L >= 0.15)', flush=True)
          print('=' * 60, flush=True)
          PYEOF
          python3 -u /tmp/bench.py
        resources:
          limits:
            nvidia.com/gpu: "1"
            memory: "48Gi"
          requests:
            nvidia.com/gpu: "1"
            memory: "24Gi"
        env:
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token
              key: HF_TOKEN
        - name: HF_HOME
          value: /cache
        - name: TRANSFORMERS_CACHE
          value: /cache
        - name: SAMPLE_SPLIT
          value: "SAMPLE_SPLIT_PLACEHOLDER"
        - name: PYTHONUNBUFFERED
          value: "1"
        volumeMounts:
        - name: cache
          mountPath: /cache
      volumes:
      - name: cache
        hostPath:
          path: /data/hf-cache
          type: DirectoryOrCreate
EOFYAML
)
    # Replace placeholder with actual split
    MLPERF_YAML="${MLPERF_YAML//SAMPLE_SPLIT_PLACEHOLDER/$SAMPLE_SPLIT}"
    if run_job "mlperf-bench" "MLPerf Inference" "$MLPERF_YAML"; then
        RESULTS+=("MLPerf: PASS ✓")
    else
        RESULTS+=("MLPerf: FAIL ✗")
    fi
    echo ""
fi

# MMLU-Pro - Official TIGER-Lab implementation with vLLM
# Based on: mmlu_pro/evaluate_from_local.py (Chain-of-Thought evaluation)
if [ "$RUN_MMLU" = true ]; then
    MMLU_YAML=$(cat <<EOF
apiVersion: batch/v1
kind: Job
metadata:
  name: mmlu-bench
  namespace: mlperf
spec:
  backoffLimit: 0
  template:
    spec:
      restartPolicy: Never
      runtimeClassName: nvidia
      nodeSelector:
        nvidia.com/gpu.present: "true"
      containers:
      - name: bench
        image: vllm/vllm-openai:v0.6.6
        command: ["bash", "-c"]
        args:
        - |
          set -e
          echo "========================================"
          echo " MMLU-Pro - Llama 3.1 8B"
          echo " (Official TIGER-Lab evaluation with vLLM)"
          echo "========================================"
          
          pip install -q datasets tqdm
          
          python3 -u << 'PY'
import time
import re
import random
import torch
from collections import defaultdict
from datasets import load_dataset
from vllm import LLM, SamplingParams

random.seed(12345)

CHOICES = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J"]

print("=" * 60, flush=True)
print("MMLU-Pro Benchmark - Official TIGER-Lab Evaluation", flush=True)
print("Model: meta-llama/Llama-3.1-8B-Instruct", flush=True)
print("Backend: vLLM (batched inference)", flush=True)
print("Method: Chain-of-Thought (CoT) prompting", flush=True)
print("=" * 60, flush=True)

# Load dataset
print("\n[1/4] Loading MMLU-Pro dataset...", flush=True)
load_start = time.time()
dataset = load_dataset("TIGER-Lab/MMLU-Pro")
test_data = list(dataset["test"]${SAMPLE_SPLIT})
val_data = list(dataset["validation"])
total = len(test_data)
print(f"  Test set: {total} questions", flush=True)
print(f"  Validation set: {len(val_data)} questions (for few-shot)", flush=True)
print(f"  Loaded in {time.time()-load_start:.1f}s", flush=True)

# Preprocess: remove N/A options
def preprocess(data):
    for item in data:
        item["options"] = [o for o in item["options"] if o != "N/A"]
    return data

test_data = preprocess(test_data)
val_data = preprocess(val_data)

# Load model with vLLM
print("\n[2/4] Loading model with vLLM...", flush=True)
model_start = time.time()
llm = LLM(
    model="meta-llama/Llama-3.1-8B-Instruct",
    dtype="float16",
    gpu_memory_utilization=0.9,
    max_model_len=4096,
    trust_remote_code=True
)
print(f"  Model loaded in {time.time()-model_start:.1f}s", flush=True)
print(f"  GPU: {torch.cuda.get_device_name(0)}", flush=True)

# Official TIGER-Lab CoT prompt format
def format_cot_example(example, include_answer=True):
    prompt = "Question:\n" + example["question"] + "\nOptions:\n"
    for i, opt in enumerate(example["options"]):
        prompt += f"{CHOICES[i]}. {opt}\n"
    if include_answer:
        cot = example.get("cot_content", "")
        if cot:
            cot = cot.replace("A: Let's think step by step.", "Answer: Let's think step by step.")
            prompt += cot + "\n\n"
    else:
        prompt += "Answer: Let's think step by step."
    return prompt

def build_few_shot_prompt(val_data, curr, k=5):
    """Build few-shot prompt with examples from same category."""
    category = curr["category"]
    same_cat = [v for v in val_data if v["category"] == category][:k]
    
    prompt = f"The following are multiple choice questions about {category}. Think step by step and then output the answer in the format of \"The answer is (X)\" at the end.\n\n"
    for ex in same_cat:
        prompt += format_cot_example(ex, include_answer=True)
    prompt += format_cot_example(curr, include_answer=False)
    return prompt

# Official answer extraction (from mmlu_pro/evaluate_from_local.py)
def extract_answer(text):
    # Try "answer is (X)" pattern first
    match = re.search(r"answer is \(?([A-J])\)?", text, re.IGNORECASE)
    if match:
        return match.group(1).upper()
    # Try "Answer: X" pattern
    match = re.search(r'[aA]nswer:\s*([A-J])', text)
    if match:
        return match.group(1).upper()
    # Last resort: find last letter A-J
    match = re.search(r"\b([A-J])\b(?!.*\b[A-J]\b)", text, re.DOTALL)
    if match:
        return match.group(0).upper()
    return None

# Build all prompts
print("\n[3/4] Running CoT inference with vLLM batching...", flush=True)
prompts = []
for item in test_data:
    prompts.append(build_few_shot_prompt(val_data, item, k=5))

# vLLM batch inference
sampling_params = SamplingParams(
    temperature=0,
    max_tokens=1024,
    stop=["Question:"]
)

t0 = time.time()
batch_size = 16
predictions = []
for i in range(0, len(prompts), batch_size):
    batch = prompts[i:i+batch_size]
    outputs = llm.generate(batch, sampling_params)
    for output in outputs:
        text = output.outputs[0].text
        pred = extract_answer(text)
        predictions.append(pred)
    
    done = min(i + batch_size, len(prompts))
    elapsed = time.time() - t0
    rate = done / elapsed if elapsed > 0 else 0
    eta = (total - done) / rate / 60 if rate > 0 else 0
    pct = done / total * 100
    
    # Calculate running accuracy
    correct_so_far = sum(1 for j in range(done) if predictions[j] == test_data[j]["answer"])
    acc_so_far = correct_so_far / done if done > 0 else 0
    
    print(f"  [{done}/{total}] {pct:.1f}% | Acc: {acc_so_far:.1%} | {rate:.1f} q/s | ETA: {eta:.0f}m", flush=True)

elapsed = time.time() - t0

# Calculate results by category (official format)
print("\n[4/4] Computing results...", flush=True)
category_stats = defaultdict(lambda: {"correct": 0, "total": 0})
correct = 0
for i, item in enumerate(test_data):
    pred = predictions[i]
    is_correct = (pred == item["answer"])
    if is_correct:
        correct += 1
        category_stats[item["category"]]["correct"] += 1
    category_stats[item["category"]]["total"] += 1

acc = correct / total

print(f"\n{'='*60}", flush=True)
print("RESULTS (Official TIGER-Lab Metrics)", flush=True)
print(f"{'='*60}", flush=True)
print("\nCategory-level accuracy:", flush=True)
for cat, stats in sorted(category_stats.items()):
    cat_acc = stats["correct"] / stats["total"] if stats["total"] > 0 else 0
    print(f"  {cat}: {cat_acc:.1%} ({stats['correct']}/{stats['total']})", flush=True)

print(f"\n{'='*60}", flush=True)
print(f"Overall Accuracy: {acc:.2%} ({correct}/{total})", flush=True)
print(f"Time: {elapsed/60:.1f} minutes", flush=True)
print(f"Throughput: {total/elapsed:.1f} questions/s", flush=True)
print(f"{'='*60}", flush=True)
status = "PASS" if acc >= 0.35 else "FAIL"
print(f"Status: {status} (Accuracy {'≥' if acc >= 0.35 else '<'} 35%)", flush=True)
print(f"{'='*60}", flush=True)
PY
        resources:
          limits: { nvidia.com/gpu: "1", memory: "48Gi" }
          requests: { nvidia.com/gpu: "1", memory: "24Gi" }
        env:
        - name: HF_TOKEN
          valueFrom: { secretKeyRef: { name: hf-token, key: HF_TOKEN } }
        - name: HF_HOME
          value: /cache
        - name: TRANSFORMERS_CACHE
          value: /cache
        - name: VLLM_USAGE_SOURCE
          value: "production"
        - name: PYTHONUNBUFFERED
          value: "1"
        volumeMounts:
        - { name: cache, mountPath: /cache }
      volumes:
      - name: cache
        hostPath: { path: /data/hf-cache, type: DirectoryOrCreate }
EOF
)
    if run_job "mmlu-bench" "MMLU-Pro Benchmark" "$MMLU_YAML"; then
        RESULTS+=("MMLU-Pro: PASS ✓")
    else
        RESULTS+=("MMLU-Pro: FAIL ✗")
    fi
    echo ""
fi

# Inference - vLLM throughput test
if [ "$RUN_INFERENCE" = true ]; then
    INF_YAML=$(cat <<EOF
apiVersion: batch/v1
kind: Job
metadata:
  name: inference-bench
  namespace: mlperf
spec:
  backoffLimit: 0
  template:
    spec:
      restartPolicy: Never
      runtimeClassName: nvidia
      nodeSelector:
        nvidia.com/gpu.present: "true"
      containers:
      - name: bench
        image: vllm/vllm-openai:v0.6.6
        command: ["bash", "-c"]
        args:
        - |
          set -e
          echo "========================================"
          echo " LLM Inference Test - Llama 3.1 8B"
          echo " (vLLM backend)"
          echo "========================================"
          
          python3 -u << 'PY'
import time
import torch
from vllm import LLM, SamplingParams

print("=" * 60, flush=True)
print("LLM Inference Benchmark", flush=True)
print("Model: meta-llama/Llama-3.1-8B-Instruct", flush=True)
print("Backend: vLLM", flush=True)
print("=" * 60, flush=True)

# Load model
print("\n[1/3] Loading model with vLLM...", flush=True)
model_start = time.time()
llm = LLM(
    model="meta-llama/Llama-3.1-8B-Instruct",
    dtype="float16",
    gpu_memory_utilization=0.9,
    max_model_len=4096,
    trust_remote_code=True
)
print(f"  Model loaded in {time.time()-model_start:.1f}s", flush=True)
print(f"  GPU: {torch.cuda.get_device_name(0)}", flush=True)
print(f"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB total", flush=True)

# Single prompt test
print("\n[2/3] Running single prompt test...", flush=True)
prompt = "How to make spaghetti from scratch? Provide a detailed step-by-step recipe."
print(f"Prompt: {prompt}\n", flush=True)

sampling_params = SamplingParams(
    temperature=0.7,
    max_tokens=512
)

t0 = time.time()
outputs = llm.generate([prompt], sampling_params)
elapsed = time.time() - t0

response = outputs[0].outputs[0].text
tokens = len(outputs[0].outputs[0].token_ids)

print(f"Response:\n{response}\n", flush=True)
print(f"{'='*60}", flush=True)
print(f"Single Prompt Results:", flush=True)
print(f"  Tokens generated: {tokens}", flush=True)
print(f"  Time: {elapsed:.2f}s", flush=True)
print(f"  Throughput: {tokens/elapsed:.1f} tokens/s", flush=True)
print(f"{'='*60}", flush=True)

# Batch throughput test
print("\n[3/3] Running batch throughput test...", flush=True)
test_prompts = [
    "What is the capital of France?",
    "Explain quantum computing in simple terms.",
    "Write a haiku about programming.",
    "What are the benefits of exercise?",
    "How does photosynthesis work?",
    "Describe the water cycle.",
    "What is machine learning?",
    "Explain the theory of relativity.",
]

batch_params = SamplingParams(temperature=0, max_tokens=256)

t0 = time.time()
batch_outputs = llm.generate(test_prompts, batch_params)
batch_elapsed = time.time() - t0

total_tokens = sum(len(o.outputs[0].token_ids) for o in batch_outputs)
print(f"\n{'='*60}", flush=True)
print(f"Batch Throughput Results ({len(test_prompts)} prompts):", flush=True)
print(f"  Total tokens generated: {total_tokens}", flush=True)
print(f"  Time: {batch_elapsed:.2f}s", flush=True)
print(f"  Throughput: {total_tokens/batch_elapsed:.1f} tokens/s", flush=True)
print(f"  Prompts/s: {len(test_prompts)/batch_elapsed:.1f}", flush=True)
print(f"{'='*60}", flush=True)
print(f"Status: PASS", flush=True)
print(f"{'='*60}", flush=True)
PY
        resources:
          limits: { nvidia.com/gpu: "1", memory: "48Gi" }
          requests: { nvidia.com/gpu: "1", memory: "24Gi" }
        env:
        - name: HF_TOKEN
          valueFrom: { secretKeyRef: { name: hf-token, key: HF_TOKEN } }
        - name: HF_HOME
          value: /cache
        - name: TRANSFORMERS_CACHE
          value: /cache
        - name: VLLM_USAGE_SOURCE
          value: "production"
        - name: PYTHONUNBUFFERED
          value: "1"
        volumeMounts:
        - { name: cache, mountPath: /cache }
      volumes:
      - name: cache
        hostPath: { path: /data/hf-cache, type: DirectoryOrCreate }
EOF
)
    if run_job "inference-bench" "LLM Inference Test" "$INF_YAML"; then
        RESULTS+=("Inference: PASS ✓")
    else
        RESULTS+=("Inference: FAIL ✗")
    fi
    echo ""
fi

# Summary
echo "[4/4] Final Summary"
echo "╔══════════════════════════════════════════════════════════════════╗"
echo "║                    BENCHMARK RESULTS                             ║"
echo "╠══════════════════════════════════════════════════════════════════╣"
for r in "${RESULTS[@]}"; do
    printf "║  %-64s ║\n" "$r"
done
echo "╠══════════════════════════════════════════════════════════════════╣"
printf "║  %-64s ║\n" "Completed: $(date '+%Y-%m-%d %H:%M:%S')"
printf "║  %-64s ║\n" "Mode: $([ "$SMOKE_TEST" = true ] && echo 'Smoke Test' || echo 'Full Dataset')"
echo "╚══════════════════════════════════════════════════════════════════╝"
